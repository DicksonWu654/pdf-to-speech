{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "def download(lang, tgt_dir=\"./\"):\n",
    "  lang_fn, lang_dir = os.path.join(tgt_dir, lang+'.tar.gz'), os.path.join(tgt_dir, lang)\n",
    "  cmd = \";\".join([\n",
    "        f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
    "        f\"tar zxvf {lang_fn}\"\n",
    "  ])\n",
    "  print(f\"Download model for language: {lang}\")\n",
    "\n",
    "  print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
    "  return lang_dir\n",
    "\n",
    "LANG = \"eng\"\n",
    "ckpt_dir = download(LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we need to work from vits\n",
    "import sys \n",
    "\n",
    "sys.path.append(\"../vits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell generates a lot of verbose - literally crashes jupyter. Careful lol\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import commons\n",
    "import utils\n",
    "import argparse\n",
    "import subprocess\n",
    "from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    print(lang)\n",
    "    if lang == 'ron':\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, \\\n",
    "             tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model)\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Hello there, Testing direct text to speech\"\n",
    "\n",
    "print(f\"text: {txt}\")\n",
    "txt = preprocess_text(txt, text_mapper, hps, lang=LANG)\n",
    "stn_tst = text_mapper.get_text(txt, hps)\n",
    "with torch.no_grad():\n",
    "    x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "    hyp = net_g.infer(\n",
    "        x_tst, x_tst_lengths, noise_scale=.667,\n",
    "        noise_scale_w=0.8, length_scale=1.0\n",
    "    )[0][0,0].cpu().float().numpy()\n",
    "\n",
    "print(f\"Generated audio\")\n",
    "# Audio(hyp, rate=hps.data.sampling_rate)\n",
    "\n",
    "#turn hyp into wav\n",
    "hyp = hyp * 32768\n",
    "hyp = hyp.astype(np.int16)\n",
    "write('test.wav', hps.data.sampling_rate, hyp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdftospeech2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
