{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to iterate through each page:\n",
    "def get_text_from_pages(file_path, start_page, end_page):\n",
    "    page_texts = []\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in range(start_page, end_page):\n",
    "            text = reader.pages[page].extract_text()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            page_texts.append(text)\n",
    "    \n",
    "    return page_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pdfs/Moby_Dick.pdf\"\n",
    "start_page = 19\n",
    "end_page = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download model for language: eng\n",
      "Model checkpoints in ./eng: ['G_100000.pth', 'config.json', 'vocab.txt']\n",
      "Run inference with cuda\n",
      "load ./eng/G_100000.pth\n",
      "INFO:root:Loaded checkpoint './eng/G_100000.pth' (iteration 6251)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../vits\")\n",
    "\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import commons\n",
    "import utils\n",
    "import subprocess\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download(lang, tgt_dir=\"./\"):\n",
    "  lang_fn, lang_dir = os.path.join(tgt_dir, lang+'.tar.gz'), os.path.join(tgt_dir, lang)\n",
    "  cmd = \";\".join([\n",
    "        f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
    "        f\"tar zxvf {lang_fn}\"\n",
    "  ])\n",
    "  print(f\"Download model for language: {lang}\")\n",
    "\n",
    "  print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
    "  return lang_dir\n",
    "\n",
    "LANG = \"eng\"\n",
    "ckpt_dir = download(LANG)\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    # print(lang)\n",
    "    if lang == 'ron':\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, \\\n",
    "             tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        # print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model)\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_texts = get_text_from_pages(file_path, start_page, end_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done page 19\n",
      "Done page 20\n"
     ]
    }
   ],
   "source": [
    "for i, page_text in enumerate(page_texts):\n",
    "    txt = preprocess_text(page_text, text_mapper, hps, lang=LANG)\n",
    "    stn_tst = text_mapper.get_text(txt, hps)\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        hyp = net_g.infer(\n",
    "            x_tst, x_tst_lengths, noise_scale=.667,\n",
    "            noise_scale_w=0.8, length_scale=1.0\n",
    "        )[0][0,0].cpu().float().numpy()\n",
    "    \n",
    "    hyp = hyp * 32768\n",
    "    hyp = hyp.astype(np.int16)\n",
    "\n",
    "    page_number = start_page + i\n",
    "\n",
    "    write(f'outputs/audio_{page_number}.wav', hps.data.sampling_rate, hyp)\n",
    "    print(f\"Done page {page_number}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdftospeech2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
