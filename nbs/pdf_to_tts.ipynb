{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to iterate through each page:\n",
    "def get_text_from_pages(file_path, start_page, end_page):\n",
    "    page_texts = []\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in range(start_page, end_page):\n",
    "            text = reader.pages[page].extract_text()\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            page_texts.append(text)\n",
    "    \n",
    "    return page_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"pdfs/Moby_Dick.pdf\"\n",
    "start_page = 19\n",
    "end_page = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download model for language: eng\n",
      "Model checkpoints in ./eng: ['G_100000.pth', 'config.json', 'vocab.txt']\n",
      "Run inference with cuda\n",
      "load ./eng/G_100000.pth\n",
      "INFO:root:Loaded checkpoint './eng/G_100000.pth' (iteration 6251)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../vits\")\n",
    "\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import torch\n",
    "import numpy as np\n",
    "import commons\n",
    "import utils\n",
    "import subprocess\n",
    "from models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download(lang, tgt_dir=\"./\"):\n",
    "  lang_fn, lang_dir = os.path.join(tgt_dir, lang+'.tar.gz'), os.path.join(tgt_dir, lang)\n",
    "  cmd = \";\".join([\n",
    "        f\"wget https://dl.fbaipublicfiles.com/mms/tts/{lang}.tar.gz -O {lang_fn}\",\n",
    "        f\"tar zxvf {lang_fn}\"\n",
    "  ])\n",
    "  print(f\"Download model for language: {lang}\")\n",
    "\n",
    "  print(f\"Model checkpoints in {lang_dir}: {os.listdir(lang_dir)}\")\n",
    "  return lang_dir\n",
    "\n",
    "LANG = \"eng\"\n",
    "ckpt_dir = download(LANG)\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    print(lang)\n",
    "    if lang == 'ron':\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, \\\n",
    "             tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model)\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_texts = get_text_from_pages(file_path, start_page, end_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('outputs'):\n",
    "    os.makedirs('outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "text after filtering OOV: moby dick  chapter i loomings  call meishmael some years ago never mind how long precisely having little ornomoney inmy purse and noth ing particular tointerest me on shore ithought iwould sail about alittle and see the watery part ofthe world it isaway ihave ofdriving offthe spleen and regulating the circulation whenever ifind myself growing grim about the mouth whenever itisadamp drizzly november in my soul whenever ifind myself involuntarily pausing be fore coffin warehouses and bringing upthe rear ofevery funeral imeet and especially whenever my hypos get such an upper hand ofmethat itrequires astrong moral principle toprevent me from deliberately stepping into the street and methodically knocking people's hats offthen  iaccount ithigh time toget tosea assoon asican this ismy substitute for pistol and ball with aphilosophical flourish cato throws himself upon his sword iquietly take tothe ship there isnothing surprising inthis ifthey but knew italmost allmen intheir degree some time or other cherish very nearly the same feelings towards theocean with me there now isyour insular city ofthe manhattoes belted round bywharves asindian isles bycoral reefs commerce surrounds itwith her surf right and left the streets take you waterward its extreme down -town isthe battery  where that noble mole iswashed by waves and cooled by breezes which afew hours previous were out ofsight of land look atthe crowds ofwater -gazers there  circumambulate the city ofadreamy sabbath afternoon  gofrom corlears hook tocoenties slip and from thence  bywhitehall northward what doyou see posted like\n",
      "eng\n",
      "text after filtering OOV:  moby dick  silent sentinels allaround the town stand thousands upon thousands ofmortal men fixed inocean reveries some leaning against the spiles some seated upon the pier -heads  some looking over the bulwarks ofships from china some high aloft inthe rigging asifstriving toget astill better seaward peep but these are all landsmen ofweek days pent up in lath and plaster tied tocounters nailed to benches clinched todesks how then isthis are the green fields gone what dothey here  but look here come more crowds pacing straight forthe water and seemingly bound for adive strange noth ing will content them but the extremest limit ofthe land  loitering under the shady lee ofyonder warehouses will not suffice no they must get just asnigh the water asthey possibly can without falling inand there they stand miles ofthem leagues inlanders allthey come from lanes and alleys streets and avenues north east south and west  yet here they allunite tell medoes the magnetic virtue ofthe needles ofthe compasses ofall those ships attract them thither  once more say you are inthe country insome high land oflakes take almost any path you please and ten toone itcarries you down inadale and leaves you there byapool inthe stream there ismagic initlet the most absent -minded ofmen beplunged inhis deepest rev eries stand that man onhis legs set his feet a-going and hewill infallibly lead you towater ifwater there beinall that region should you ever beathirst inthe great amer ican desert try this experiment ifyour caravan happen to be supplied with ametaphysical professor yes asevery one knows meditation and water are wedded forever  but here isanartist hedesires topaint you the dream iest shadiest quietest most enchanting bit ofromantic landscape inallthe valley ofthe saco what isthe chief element he employs there stand his trees each with a hollow trunk asifahermit and acrucifix were within and here sleeps his meadow and there sleep his cattle and up from yonder cottage goes asleepy smoke deep into dis tant woodlands winds amazy way reaching tooverlapping spurs ofmountains bathed intheir hill-side blue but though the picture lies thus tranced and though this pine tree shakes down itssighs like leaves upon this shepherd's head yet allwere vain unless the shepherd's eye were fixed upon the magic stream before him govisit the prairies in \n"
     ]
    }
   ],
   "source": [
    "for i, page_text in enumerate(page_texts):\n",
    "    txt = preprocess_text(page_text, text_mapper, hps, lang=LANG)\n",
    "    stn_tst = text_mapper.get_text(txt, hps)\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        hyp = net_g.infer(\n",
    "            x_tst, x_tst_lengths, noise_scale=.667,\n",
    "            noise_scale_w=0.8, length_scale=1.0\n",
    "        )[0][0,0].cpu().float().numpy()\n",
    "    \n",
    "    hyp = hyp * 32768\n",
    "    hyp = hyp.astype(np.int16)\n",
    "    write(f'outputs/audio_{start_page+i}.wav', hps.data.sampling_rate, hyp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdftospeech2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
